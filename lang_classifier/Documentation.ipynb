{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "895d21bd",
   "metadata": {},
   "source": [
    "# Language Classifier Model\n",
    "\n",
    "---\n",
    "\n",
    "This is a ML model created by Keshav Ghai (An aspiring AI/ML dev).\n",
    "It is basic language classifier which can successfully differ between 3 languages, English, Hindi and Punjabi. This model features a tensorflow framework, utilising sklearn metrics for support. The training script **\"model_trainer.py\"** takes training data, puts it through a rigorous process of calculations *(Explained below)* and returns a trained model and a few graphs to understand how well the training of the model went. The model is trained on 100K+ parameters and a dataset which was handpicked and cleaned.\n",
    "\n",
    "## Imports:- \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b353491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a9c42d",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Loading the Dataset (.txt in current directory)\n",
    "---\n",
    "\n",
    "\n",
    "> The dataset is loaded from **\"dataset.txt\"** using fileread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3946ac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = \"dataset.txt\"   # Name of your file in same folder\n",
    "assert os.path.exists(DATA_FILE), \"Dataset file not found!\"\n",
    "\n",
    "texts = []\n",
    "labels = []\n",
    "\n",
    "with open(DATA_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # Expected format: \"text ### label\"\n",
    "        if \"###\" in line:\n",
    "            text, label = line.split(\"###\")\n",
    "            texts.append(text.strip())\n",
    "            labels.append(label.strip())\n",
    "        else:\n",
    "            print(\"Skipping line (wrong format):\", line)\n",
    "\n",
    "# Encode labels\n",
    "label_to_id = {\"english\": 0, \"hindi\": 1, \"punjabi\": 2}\n",
    "y = np.array([label_to_id[l.lower()] for l in labels])\n",
    "\n",
    "print(\"Loaded samples:\", len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d0cadf",
   "metadata": {},
   "source": [
    "## 2. Character Tokenizer \n",
    "---\n",
    "\n",
    "> The tokenizer is setup and text is converted into char indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bff3b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    char_level=True,\n",
    "    lower=True,\n",
    "    filters=\"\"\n",
    ")\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Save tokenizer\n",
    "with open(\"char_tokenizer.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(tokenizer.to_json(), ensure_ascii=False))\n",
    "\n",
    "print(\"Tokenizer saved as char_tokenizer.json\")\n",
    "\n",
    "# Convert text â†’ char indices\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Pad sequences\n",
    "max_len = max(len(seq) for seq in sequences)\n",
    "X = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29af4dd0",
   "metadata": {},
   "source": [
    "## 3. Defining the Model's dimensions (FP32, 100K parameters)\n",
    "---\n",
    "\n",
    "> The model is created with 6 layers. The specifications of these layers can be changed later. (Just make sure it doesn't overfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b30100",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(max_len,), dtype=\"int32\"),\n",
    "    tf.keras.layers.Embedding(vocab_size, 128),       # char embedding\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(3, activation=\"softmax\", dtype=\"float32\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary() # This shows you the model's details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a006008e",
   "metadata": {},
   "source": [
    "## 4. Training the model (No validation)\n",
    "---\n",
    "> The model is fitted over 20 epochs and a batch size of 16. (These settings are sensitive, so only change them if you know what you are doing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7d946b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X, y,\n",
    "    epochs=20,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "model.save(\"language_classifier.keras\")\n",
    "print(\"\\nModel saved as language_classifier.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ace4da",
   "metadata": {},
   "source": [
    "## 5. Graphs\n",
    "---\n",
    "> Multiple graphs are created to visualize what the model is doing, how much we lost during training and more details. (Good for learning about ML)\n",
    "\n",
    "### a. Loss Over Epochs:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a9e1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title(\"Training Loss Over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.savefig(\"loss_graph.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bd7203",
   "metadata": {},
   "source": [
    "### b. Train vs. Test Accuracy:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e3bdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = history.history['accuracy']\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(train_acc, label=\"Training Accuracy\")\n",
    "plt.title(\"Accuracy on Training Data\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid(True)\n",
    "plt.savefig(\"train_accuracy_graph.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8d8fe5",
   "metadata": {},
   "source": [
    "### c. Confusion Matrix:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70d309f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X)\n",
    "pred_labels = np.argmax(pred, axis=1)\n",
    "\n",
    "cm = confusion_matrix(y, pred_labels)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\",\n",
    "            xticklabels=[\"English\", \"Hindi\", \"Punjabi\"],\n",
    "            yticklabels=[\"English\", \"Hindi\", \"Punjabi\"])\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.savefig(\"confusion_matrix.png\")\n",
    "plt.close()\n",
    "\n",
    "print(\"Saved: loss_graph.png, train_accuracy_graph.png, confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79395f93",
   "metadata": {},
   "source": [
    "## 6. Interactive Testing\n",
    "---\n",
    "> This is a basic test for the model. It checks whether the model is predicting languages correctly or not. (Remember to use this thoroughly if you tampered with the specifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b3b152",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    user_inp = input(\"\\nEnter text to classify (or 'quit'): \").strip()\n",
    "    if user_inp.lower() == \"quit\":\n",
    "        break\n",
    "\n",
    "    seq = tokenizer.texts_to_sequences([user_inp])\n",
    "    seq = tf.keras.preprocessing.sequence.pad_sequences(seq, maxlen=max_len)\n",
    "\n",
    "    pred = model.predict(seq)[0]\n",
    "    idx = np.argmax(pred)\n",
    "\n",
    "    for lang, id_ in label_to_id.items():\n",
    "        if id_ == idx:\n",
    "            print(\"Predicted Language:\", lang.capitalize())\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
