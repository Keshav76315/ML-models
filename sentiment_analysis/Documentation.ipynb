{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e25a98db",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Model\n",
    "\n",
    "---\n",
    "\n",
    "This is a ML model created by Keshav Ghai (An aspiring AI/ML dev).\n",
    "It is a sentiment classifier which can successfully classify text into 3 sentiments: Positive, Neutral, and Negative. This model features a tensorflow framework, utilising sklearn metrics for support. The training script **\"trainer.py\"** takes training data from a CSV file, puts it through a rigorous process of calculations *(Explained below)* and returns a trained model and a few graphs to understand how well the training of the model went. The model is trained on word-level embeddings and a pre-written dataset from Kagglehub.\n",
    "\n",
    "## Imports:- \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf652a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894bb638",
   "metadata": {},
   "source": [
    "## 1. Loading the Dataset (CSV in current directory)\n",
    "---\n",
    "\n",
    "> The dataset is loaded from **\"dataset.csv\"** using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca70159",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"./tensorflow/sentiment_analysis/dataset.csv\"\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "\n",
    "# Expected columns -> \"text\", \"label\"\n",
    "assert \"text\" in df.columns and \"label\" in df.columns, \"CSV must contain 'text' and 'label' columns\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29c2fae",
   "metadata": {},
   "source": [
    "## 2. Label Encoding (positive/neutral/negative)\n",
    "---\n",
    "\n",
    "> Labels are encoded to numeric values for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd03224",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df[\"label_id\"] = le.fit_transform(df[\"label\"])\n",
    "\n",
    "print(\"Label mapping:\", dict(zip(le.classes_, le.transform(le.classes_))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd83d8b",
   "metadata": {},
   "source": [
    "## 3. Train / Validation Split\n",
    "---\n",
    "\n",
    "> Data is split into 85% training and 15% validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361d4264",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.sample(frac=0.85, random_state=42)\n",
    "val_df = df.drop(train_df.index)\n",
    "\n",
    "print(\"Train size:\", train_df.shape)\n",
    "print(\"Validation size:\", val_df.shape)\n",
    "\n",
    "# Extract values\n",
    "train_texts = train_df[\"text\"].astype(str).tolist()\n",
    "train_labels = train_df[\"label_id\"].tolist()\n",
    "\n",
    "val_texts = val_df[\"text\"].astype(str).tolist()\n",
    "val_labels = val_df[\"label_id\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013154d2",
   "metadata": {},
   "source": [
    "## 4. Word-Level Tokenizer\n",
    "---\n",
    "\n",
    "> The tokenizer is setup and text is converted into word indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67045e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    char_level=False,\n",
    "    lower=True,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    ")\n",
    "\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "# Save tokenizer\n",
    "with open(\"word_tokenizer.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(tokenizer.to_json(), ensure_ascii=False))\n",
    "\n",
    "print(\"Tokenizer saved as word_tokenizer.json\")\n",
    "\n",
    "# Convert text â†’ sequences\n",
    "train_seq = tokenizer.texts_to_sequences(train_texts)\n",
    "val_seq = tokenizer.texts_to_sequences(val_texts)\n",
    "\n",
    "# Padding\n",
    "max_len = 256\n",
    "train_pad = tf.keras.preprocessing.sequence.pad_sequences(train_seq, maxlen=max_len)\n",
    "val_pad = tf.keras.preprocessing.sequence.pad_sequences(val_seq, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d4b4bb",
   "metadata": {},
   "source": [
    "## 5. Defining the Model's dimensions\n",
    "---\n",
    "\n",
    "> The model is created with 5 layers. The specifications of these layers can be changed later. (Just make sure it doesn't overfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0060693d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(max_len,), dtype=\"int32\"),\n",
    "    tf.keras.layers.Embedding(vocab_size, 4),            # word embeddings\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)),\n",
    "    tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(3, activation=\"softmax\", dtype=\"float32\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b004c6",
   "metadata": {},
   "source": [
    "## 6. Training the model (With validation)\n",
    "---\n",
    "\n",
    "> The model is fitted over 6 epochs and a batch size of 32. (These settings are sensitive, so only change them if you know what you are doing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6990ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_pad, np.array(train_labels),\n",
    "    validation_data=(val_pad, np.array(val_labels)),\n",
    "    epochs=6,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.save(\"./Models/sentiment_model.keras\")\n",
    "print(\"Model saved as sentiment_model.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdb64b9",
   "metadata": {},
   "source": [
    "## 7. Graphs\n",
    "---\n",
    "\n",
    "> Multiple graphs are created to visualize what the model is doing, how much we lost during training and more details. (Good for learning about ML)\n",
    "\n",
    "### a. Loss Over Epochs:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ba0f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Val Loss\")\n",
    "plt.title(\"Loss Over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"./tensorflow/sentiment_analysis/loss_graph.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1c381d",
   "metadata": {},
   "source": [
    "### b. Train vs. Validation Accuracy:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77960284",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(history.history[\"accuracy\"], label=\"Train Accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"Val Accuracy\")\n",
    "plt.title(\"Accuracy Over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"./tensorflow/sentiment_analysis/accuracy_graph.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0230005c",
   "metadata": {},
   "source": [
    "### c. Confusion Matrix:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800a36bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = model.predict(val_pad)\n",
    "val_pred = np.argmax(val_pred, axis=1)\n",
    "\n",
    "cm = confusion_matrix(val_labels, val_pred)\n",
    "\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\",\n",
    "            xticklabels=le.classes_,\n",
    "            yticklabels=le.classes_)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.savefig(\"./tensorflow/sentiment_analysis/confusion_matrix.png\")\n",
    "plt.close()\n",
    "\n",
    "print(\"Saved: loss_graph.png, accuracy_graph.png, confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619a4b02",
   "metadata": {},
   "source": [
    "## 8. Interactive Testing\n",
    "---\n",
    "\n",
    "> This is a basic test for the model. It checks whether the model is predicting sentiments correctly or not. (Remember to use this thoroughly if you tampered with the specifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e13e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_vals = ['Negative', 'Neutral', 'Positive']\n",
    "\n",
    "print(\"\\nInteractive testing mode:\")\n",
    "while True:\n",
    "    text = input(\"Enter text (or 'quit'): \")\n",
    "    if text.lower() == \"quit\":\n",
    "        break\n",
    "\n",
    "    seq = tokenizer.texts_to_sequences([text])\n",
    "    seq = tf.keras.preprocessing.sequence.pad_sequences(seq, maxlen=max_len)\n",
    "\n",
    "    pred = model.predict(seq)[0]\n",
    "    label_id = np.argmax(pred)\n",
    "    label = le.inverse_transform([label_id])[0]\n",
    "\n",
    "    print(\"Predicted Sentiment:\", label_vals[label_id])\n",
    "    print(f\"Confidence: {pred[label_id]*100:.2f}%\")\n",
    "    for i, prob in enumerate(pred):\n",
    "        print(f\"    {label_vals[i]}: {prob*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
