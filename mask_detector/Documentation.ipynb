{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8151c5e9",
   "metadata": {},
   "source": [
    "# Mask Detection CNN Model\n",
    "\n",
    "---\n",
    "\n",
    "This is a ML model created by Keshav Ghai (An aspiring AI/ML dev).\n",
    "\n",
    "This is a **Convolutional Neural Network (CNN)** which detects whether a person is wearing a face mask or not from image data. Unlike previous models that worked with text or tabular data, this model learns visual patterns directly from raw image pixels. The training script **\"CNN_trainer.py\"** takes image data, preprocesses it, trains a CNN with 3 convolutional layers, and generates visualizations.\n",
    "\n",
    "## What makes this different?\n",
    "\n",
    "CNNs are special neural networks designed for image data. They use **convolutional layers** that automatically learn to detect features like edges, textures, and shapes by sliding small filters across images. This is much more effective than flattening images into vectors because it preserves spatial relationships.\n",
    "\n",
    "## Imports:-\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4e8efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import cv2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e302e83",
   "metadata": {},
   "source": [
    "## 0. Dataset Normalization (Data Preprocessing)\n",
    "---\n",
    "\n",
    "Before actually starting the model, we have to normalize the dataset. It is done through these steps:\n",
    "\n",
    "**Why normalize?** Raw images come in different sizes and formats. Normalization standardizes them so the model can process them consistently.\n",
    "\n",
    "### Normalization Steps:\n",
    "1. **Read images** using OpenCV from the dataset folder\n",
    "2. **Convert color space** from BGR (OpenCV's default) to RGB (standard format)\n",
    "3. **Resize to 256×256** - ensures all images have the same dimensions\n",
    "4. **Save normalized images** to a new clean directory\n",
    "\n",
    "The `normalization.py` script handles this automatically, creating a `normalized_dataset` folder with properly formatted images ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15136902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization.py\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "DATASET_DIR = \"./tensorflow/mask_detector/dataset\"\n",
    "OUTPUT_DIR = \"./tensorflow/mask_detector/normalized_dataset\"\n",
    "TARGET_SIZE = (256, 256)\n",
    "\n",
    "folders = [\"with_mask\", \"without_mask\", \"eval\"]\n",
    "\n",
    "def ensure_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def process_folder(folder):\n",
    "    input_path = os.path.join(DATASET_DIR, folder)\n",
    "    output_path = os.path.join(OUTPUT_DIR, folder)\n",
    "    ensure_dir(output_path)\n",
    "\n",
    "    for img_name in os.listdir(input_path):\n",
    "        img_path = os.path.join(input_path, img_name)\n",
    "\n",
    "        # Read image with OpenCV\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            print(f\"Skipping unreadable file: {img_path}\")\n",
    "            continue\n",
    "\n",
    "        # Convert BGR → RGB\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Resize to 256×256\n",
    "        img = cv2.resize(img, TARGET_SIZE)\n",
    "\n",
    "        # Save normalized image\n",
    "        out_path = os.path.join(output_path, img_name)\n",
    "        cv2.imwrite(out_path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    print(f\"Processed folder: {folder}\")\n",
    "\n",
    "def main():\n",
    "    ensure_dir(OUTPUT_DIR)\n",
    "    for folder in folders:\n",
    "        process_folder(folder)\n",
    "\n",
    "    print(\"Normalization complete! All images resized to 256x256.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a697cf5f",
   "metadata": {},
   "source": [
    "## 1. Define Paths and Constants\n",
    "---\n",
    "\n",
    "> These variables point to the normalized dataset directories and where to save the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64e4d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to normalized dataset\n",
    "DATA_DIR = \"./tensorflow/mask_detector/normalized_dataset\"\n",
    "EVAL_DIR = \"./tensorflow/mask_detector/normalized_eval\"\n",
    "CLASSES_CSV = os.path.join(EVAL_DIR, \"classes.csv\")\n",
    "\n",
    "# Where to save the trained model\n",
    "MODEL_SAVE_PATH = \"./Models/mask_detector.keras\"\n",
    "PLOT_SAVE_DIR = \"./tensorflow/mask_detector\"\n",
    "\n",
    "# Image and training settings\n",
    "IMG_SIZE = (256, 256)  # Height x Width of images the model expects\n",
    "BATCH_SIZE = 32        # Number of images to process together\n",
    "\n",
    "print(f\"Data will be loaded from: {DATA_DIR}\")\n",
    "print(f\"Model will be saved to: {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16973f5",
   "metadata": {},
   "source": [
    "## 2. Load Dataset Using TensorFlow ImageDataset\n",
    "---\n",
    "\n",
    "> TensorFlow's `image_dataset_from_directory` automatically loads images and labels from folder structure. It assumes folders are class names (e.g., \"with_mask\", \"without_mask\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1270f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data (85% of dataset)\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    DATA_DIR,\n",
    "    labels=\"inferred\",           # Read labels from folder names\n",
    "    label_mode=\"binary\",          # Binary classification: mask vs no mask (0 or 1)\n",
    "    color_mode=\"rgb\",             # Load as RGB color images (3 channels)\n",
    "    batch_size=BATCH_SIZE,         # Process 32 images at once\n",
    "    image_size=IMG_SIZE,           # Resize all images to 256x256\n",
    "    validation_split=0.15,         # 15% for validation\n",
    "    subset=\"training\",            # This loads the training portion\n",
    "    seed=42                        # Fixed seed for reproducibility\n",
    ")\n",
    "\n",
    "# Load validation data (remaining 15%)\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    DATA_DIR,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"binary\",\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMG_SIZE,\n",
    "    validation_split=0.15,\n",
    "    subset=\"validation\",           # This loads the validation portion\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Prefetch: Load next batch while current batch is being processed (optimization)\n",
    "train_ds = train_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "print(\"Dataset loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ee6326",
   "metadata": {},
   "source": [
    "## 3. Understanding the CNN Architecture\n",
    "---\n",
    "\n",
    "> A CNN learns hierarchical features: early layers detect simple patterns (edges), middle layers combine them (shapes), and later layers recognize high-level features (faces, masks).\n",
    "\n",
    "### What each component does:\n",
    "\n",
    "**Rescaling Layer**: Converts pixel values from [0, 255] to [0, 1]. This helps the neural network learn better.\n",
    "\n",
    "**Conv2D Layers**: Apply filters that slide across the image to detect features. `Conv2D(32, (3,3))` means 32 filters of size 3×3 pixels.\n",
    "\n",
    "**MaxPooling2D**: Reduces spatial dimensions by taking the maximum value in 2×2 windows. This:\n",
    "- Makes computation faster\n",
    "- Prevents overfitting\n",
    "- Focuses on the most important features\n",
    "\n",
    "**Flatten**: Converts the 2D image data into a 1D vector so it can go through dense layers.\n",
    "\n",
    "**Dense Layers**: Traditional neural network layers that learn patterns from the features extracted by convolution.\n",
    "\n",
    "**Dropout(0.3)**: Randomly ignores 30% of neurons during training to prevent overfitting.\n",
    "\n",
    "**Output Layer**: Single neuron with sigmoid activation for binary classification (mask=1, no mask=0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec91271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the CNN model\n",
    "model = tf.keras.Sequential([\n",
    "    # Input: Image is normalized from [0, 255] to [0, 1]\n",
    "    tf.keras.layers.Rescaling(1./255, input_shape=(*IMG_SIZE, 3)),\n",
    "\n",
    "    # First convolutional block: Extract low-level features (edges, textures)\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "\n",
    "    # Second convolutional block: Combine features from first layer\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "\n",
    "    # Third convolutional block: Learn high-level patterns\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "\n",
    "    # Flatten and fully connected layers\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    \n",
    "    # Binary classification output (0 or 1)\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid', dtype=\"float32\")\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=\"adam\",                    # Adam optimizer - smart learning rate adjustment\n",
    "    loss=\"binary_crossentropy\",          # Loss function for binary classification\n",
    "    metrics=[\"accuracy\"]                  # Track accuracy during training\n",
    ")\n",
    "\n",
    "# Display model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8caf0a3",
   "metadata": {},
   "source": [
    "## 4. Training the Model\n",
    "---\n",
    "\n",
    "> The model learns by:\n",
    "1. Making predictions on training images\n",
    "2. Comparing predictions to actual labels\n",
    "3. Adjusting weights to reduce error\n",
    "4. Repeating for 10 epochs (passes through entire dataset)\n",
    "\n",
    "Each epoch takes all batches of 32 images and updates the model 10 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c660e1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_ds,                    # Training data\n",
    "    validation_data=val_ds,      # Validation data (for monitoring)\n",
    "    epochs=10                    # Number of complete passes through dataset\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(MODEL_SAVE_PATH)\n",
    "print(f\"\\nModel saved at: {MODEL_SAVE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1128b4",
   "metadata": {},
   "source": [
    "## 5. Evaluate Model Performance\n",
    "---\n",
    "\n",
    "> We test the model on both training and validation data to see:\n",
    "- **Training Accuracy**: How well it learned the training data\n",
    "- **Validation Accuracy**: How well it generalizes to unseen data (the real measure)\n",
    "\n",
    "If training accuracy is much higher than validation, the model is overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaae6fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on training set\n",
    "train_loss, train_acc = model.evaluate(train_ds)\n",
    "\n",
    "# Evaluate on validation set\n",
    "val_loss, val_acc = model.evaluate(val_ds)\n",
    "\n",
    "print(f\"\\n=== Model Performance ===\")\n",
    "print(f\"Training Accuracy:   {train_acc * 100:.2f}%\")\n",
    "print(f\"Validation Accuracy: {val_acc * 100:.2f}%\")\n",
    "print(f\"\\nTraining Loss:   {train_loss:.4f}\")\n",
    "print(f\"Validation Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f2e679",
   "metadata": {},
   "source": [
    "## 6. Visualizations\n",
    "---\n",
    "\n",
    "> Multiple graphs show how the model improved over training epochs and its actual predictions.\n",
    "\n",
    "### a. Loss Over Epochs:-\n",
    "\n",
    "Shows how the error decreased during training. Ideally, both curves should smoothly decrease, and validation loss should not increase sharply (sign of overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f32e91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(history.history[\"loss\"], label=\"Train Loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"Val Loss\")\n",
    "plt.title(\"Loss Over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(PLOT_SAVE_DIR, \"loss_graph.png\"))\n",
    "plt.close()\n",
    "\n",
    "print(\"Loss graph saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a007d41",
   "metadata": {},
   "source": [
    "### b. Accuracy Over Epochs:-\n",
    "\n",
    "Shows how the model's correctness improved. A rising curve means the model is learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16463191",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(history.history[\"accuracy\"], label=\"Train Accuracy\")\n",
    "plt.plot(history.history[\"val_accuracy\"], label=\"Val Accuracy\")\n",
    "plt.title(\"Accuracy Over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(PLOT_SAVE_DIR, \"accuracy_graph.png\"))\n",
    "plt.close()\n",
    "\n",
    "print(\"Accuracy graph saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2aea6f",
   "metadata": {},
   "source": [
    "### c. Confusion Matrix:-\n",
    "\n",
    "Shows actual vs predicted labels:\n",
    "- **Top-left**: Correctly predicted \"With Mask\"\n",
    "- **Top-right**: Incorrectly said \"Without Mask\" when it was \"With Mask\" (False Negative)\n",
    "- **Bottom-left**: Incorrectly said \"With Mask\" when it was \"Without Mask\" (False Positive)\n",
    "- **Bottom-right**: Correctly predicted \"Without Mask\"\n",
    "\n",
    "Ideally, diagonal numbers should be high, off-diagonal should be low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef774e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the evaluation CSV\n",
    "df = pd.read_csv(CLASSES_CSV)\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "# Test on evaluation set\n",
    "for idx, row in df.iterrows():\n",
    "    filename = row[\"filename\"]\n",
    "    # Convert: with_mask=1 → label=0, without_mask=0 → label=1\n",
    "    true_label = 0 if row[\"with_mask\"] == 1 else 1\n",
    "\n",
    "    img_path = os.path.join(EVAL_DIR, filename)\n",
    "\n",
    "    # Read and preprocess image\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        print(f\"Skipping: {filename}\")\n",
    "        continue\n",
    "\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, IMG_SIZE)\n",
    "    img = img.astype(\"float32\") / 255.0\n",
    "    img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "\n",
    "    # Make prediction\n",
    "    pred = model.predict(img)[0][0]\n",
    "    pred_label = 1 if pred > 0.5 else 0\n",
    "\n",
    "    y_true.append(true_label)\n",
    "    y_pred.append(pred_label)\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(\n",
    "    cm, annot=True, fmt=\"d\",\n",
    "    xticklabels=[\"With Mask\", \"Without Mask\"],\n",
    "    yticklabels=[\"With Mask\", \"Without Mask\"]\n",
    ")\n",
    "plt.title(\"Confusion Matrix (Eval Set)\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.savefig(os.path.join(PLOT_SAVE_DIR, \"confusion_matrix.png\"))\n",
    "plt.close()\n",
    "\n",
    "print(\"Confusion matrix saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec511266",
   "metadata": {},
   "source": [
    "## 7. Interactive Testing Mode\n",
    "---\n",
    "\n",
    "> Test the trained model on any image. The model will:\n",
    "1. Read the image file\n",
    "2. Preprocess it (resize to 256×256, convert to RGB, normalize)\n",
    "3. Make a prediction\n",
    "4. Output \"WITH Mask\" or \"WITHOUT Mask\"\n",
    "\n",
    "The prediction value (0 to 1) closer to 0 means higher confidence for \"WITH Mask\", closer to 1 means \"WITHOUT Mask\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36e3a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nInteractive testing mode. Enter image path or 'quit' to exit.\")\n",
    "\n",
    "while True:\n",
    "    path = input(\"\\nImage path: \").strip()\n",
    "\n",
    "    if path.lower() == \"quit\":\n",
    "        print(\"Exiting testing mode.\")\n",
    "        break\n",
    "\n",
    "    # Validate path\n",
    "    if not os.path.exists(path):\n",
    "        print(\"Invalid path. Try again.\")\n",
    "        continue\n",
    "\n",
    "    # Preprocess image\n",
    "    img = cv2.imread(path)\n",
    "    if img is None:\n",
    "        print(\"Could not read image. Try another file.\")\n",
    "        continue\n",
    "        \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, IMG_SIZE)\n",
    "    img = img.astype(\"float32\") / 255.0\n",
    "    img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "\n",
    "    # Make prediction\n",
    "    pred = model.predict(img)[0][0]\n",
    "    label = \"WITHOUT Mask\" if pred > 0.5 else \"WITH Mask\"\n",
    "    confidence = pred if pred > 0.5 else (1 - pred)\n",
    "\n",
    "    print(f\"Prediction: {label}\")\n",
    "    print(f\"Confidence: {confidence * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
