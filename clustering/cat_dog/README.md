# Clustering (Cats vs Dogs) — Project Overview

This folder contains experiments and notebooks for unsupervised / semi-supervised clustering on a cats-vs-dogs image dataset.
The primary working scripts are `semi_trainer.py` and `semi_cluster.py`. Other scripts are exploratory/experimental and are labeled as such.

---

## Working / Recommended

- `semi_trainer.py` — Semi-supervised training pipeline that prepares embeddings and trains a lightweight encoder. Use this to generate feature embeddings used by the clustering pipeline.

- `semi_cluster.py` — Clustering pipeline that performs dimensionality reduction (PCA / t-SNE) and KMeans clustering. This is the main script used to produce the notebooks' results and visualizations.

- `documentation.ipynb` — Full notebook documenting the workflow (normalization → PCA → KMeans → evaluation → t-SNE). Use this for step-by-step explanation and visualizations.

---

## Normalization / Data

- `normalization.py` — Image normalization (BGR→RGB, resize to 224×224) used to build `normalized_dataset/`.
- `dataset/` — Raw images (not tracked here). After running `normalization.py`, `normalized_dataset/` is created.
- `normalized_dataset/` — Normalized images used by the trainer and clustering scripts.

---

## Outputs & Visuals

- `pca_variance.png`, `tsne_visualization.png`, `elbow_method.png` — Visuals generated by the notebook and scripts.
- `confusion_matrix.png`, `cluster_confusion_matrix.png` — Evaluation outputs for cluster assignments.
- `loss_graph.png`, `accuracy_graph.png` — Training graphs (if embedding trainer is run).

---

## Experimental / For Exploration (use cautiously)

These scripts were part of exploratory work and are not required to reproduce the core notebook results. They are kept for reference.

- `trainer.py` — Early/trial trainer (EXPERIMENTAL)
- `autoencoder_train.py` — Autoencoder experiments for dimensionality reduction (EXPERIMENTAL)
- `cluster.py` — Alternate clustering script / prototyping (EXPERIMENTAL)

---

## Quick Usage

1. Normalize images (creates `normalized_dataset/`):

```bash
python normalization.py
```

2. Generate embeddings (if required) and train encoder:

```bash
python semi_trainer.py
```

3. Run clustering pipeline and generate visualizations:

```bash
python semi_cluster.py
```

4. Open `documentation.ipynb` for a guided walkthrough and to reproduce plots.

---

## Notes

- The working scripts (`semi_trainer.py`, `semi_cluster.py`) are the tested/validated pipelines used to produce the notebook results. Experimental scripts are provided for reference and exploration but may be incomplete.
- For reproducibility, run the scripts in the order above and ensure the `Models/` and `normalized_dataset/` folders are present.

---

Maintainer: Keshav Ghai
